{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 241,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from nni.algorithms.compression.v2.pytorch.pruning import LinearPruner\n",
    "import nni"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris[\"data\"]\n",
    "X = Variable(torch.from_numpy(StandardScaler().fit_transform(X))).float()\n",
    "y = iris[\"target\"]\n",
    "y = Variable(torch.from_numpy(y)).long()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(4, 128),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(128, 3),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "outputs": [],
   "source": [
    "def accuracy(y_p, y_a):\n",
    "    return torch.sum(torch.argmax(y_p, dim=1) == y_a)/len(y_a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss reduction 0.5107707977294922\n",
      "accuracy before: 0.3333333432674408, accuracy after: 0.9866666793823242\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = model(X)\n",
    "    loss_start_train = loss_fn(y_pred, y)\n",
    "    acc_start_train = accuracy(y_pred, y)\n",
    "\n",
    "for epoch in range(2000):\n",
    "    y_pred = model(X)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X)\n",
    "    loss_end_train = loss_fn(y_pred, y)\n",
    "    acc_end_train = accuracy(y_pred, y)\n",
    "\n",
    "\n",
    "print(f\"loss reduction {(loss_start_train-loss_end_train)}\")\n",
    "print(f\"accuracy before: {acc_start_train}, accuracy after: {acc_end_train}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "outputs": [],
   "source": [
    "def training_func(model, optimizers, criterion, _lr_schedulers, _max_steps, max_epochs, *_, **__):\n",
    "\n",
    "    model.train()\n",
    "    total_epochs = max_epochs if max_epochs else 2000\n",
    "\n",
    "    for _ in range(total_epochs):\n",
    "        optimizers.zero_grad()\n",
    "        loss = criterion(model(X), y)\n",
    "        loss.backward()\n",
    "        optimizers.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-29 12:51:47] \u001B[33mWARNING: This compressor is not set model and config_list, waiting for reset() or pass this to scheduler.\u001B[0m\n",
      "[2022-11-29 12:51:47] \u001B[32msimulated prune 0 remain/total: 128/128\u001B[0m\n",
      "[2022-11-29 12:51:47] \u001B[32msimulated prune 2 remain/total: 3/3\u001B[0m\n",
      "[2022-11-29 12:51:49] \u001B[32msimulated prune 0 remain/total: 128/128\u001B[0m\n",
      "[2022-11-29 12:51:49] \u001B[32msimulated prune 2 remain/total: 3/3\u001B[0m\n",
      "[2022-11-29 12:51:50] \u001B[32msimulated prune 0 remain/total: 128/128\u001B[0m\n",
      "[2022-11-29 12:51:50] \u001B[32msimulated prune 2 remain/total: 3/3\u001B[0m\n",
      "[2022-11-29 12:51:52] \u001B[32msimulated prune 0 remain/total: 128/128\u001B[0m\n",
      "[2022-11-29 12:51:52] \u001B[32msimulated prune 2 remain/total: 3/3\u001B[0m\n",
      "[2022-11-29 12:51:54] \u001B[32msimulated prune 0 remain/total: 128/128\u001B[0m\n",
      "[2022-11-29 12:51:54] \u001B[32msimulated prune 2 remain/total: 3/3\u001B[0m\n",
      "[2022-11-29 12:51:55] \u001B[32msimulated prune 0 remain/total: 128/128\u001B[0m\n",
      "[2022-11-29 12:51:55] \u001B[32msimulated prune 2 remain/total: 3/3\u001B[0m\n",
      "[2022-11-29 12:51:57] \u001B[32msimulated prune 0 remain/total: 128/128\u001B[0m\n",
      "[2022-11-29 12:51:57] \u001B[32msimulated prune 2 remain/total: 3/3\u001B[0m\n",
      "[2022-11-29 12:51:59] \u001B[32msimulated prune 0 remain/total: 127/128\u001B[0m\n",
      "[2022-11-29 12:51:59] \u001B[32msimulated prune 2 remain/total: 3/3\u001B[0m\n",
      "[2022-11-29 12:52:01] \u001B[32msimulated prune 0 remain/total: 127/128\u001B[0m\n",
      "[2022-11-29 12:52:01] \u001B[32msimulated prune 2 remain/total: 3/3\u001B[0m\n",
      "[2022-11-29 12:52:03] \u001B[32msimulated prune 0 remain/total: 126/128\u001B[0m\n",
      "[2022-11-29 12:52:03] \u001B[32msimulated prune 2 remain/total: 3/3\u001B[0m\n",
      "[2022-11-29 12:52:04] \u001B[32msimulated prune 0 remain/total: 123/128\u001B[0m\n",
      "[2022-11-29 12:52:04] \u001B[32msimulated prune 2 remain/total: 3/3\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "from nni.algorithms.compression.v2.pytorch import TorchEvaluator\n",
    "\n",
    "optimizer_pruner = nni.trace(torch.optim.Adam)(model.parameters(), lr=1e-3)\n",
    "dummy_input = torch.rand(10, 4)\n",
    "\n",
    "evaluator = TorchEvaluator(\n",
    "    training_func=training_func,\n",
    "    optimizers=optimizer_pruner,\n",
    "    criterion=torch.nn.CrossEntropyLoss(),\n",
    "    dummy_input=dummy_input)\n",
    "\n",
    "config_list = [{\n",
    "    \"sparsity\": 0.5,\n",
    "    \"op_types\": [\"Linear\"]\n",
    "}]\n",
    "itpruner = LinearPruner(\n",
    "    model,\n",
    "    config_list,\n",
    "    total_iteration=10,\n",
    "    pruning_algorithm=\"level\",\n",
    "    evaluator=evaluator,\n",
    "    log_dir=\".nni_log/\")\n",
    "\n",
    "itpruner.compress()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best iteration 10\n",
      "loss change after prune -0.18073707818984985\n",
      "accuracy after prune: 0.7266666889190674\n"
     ]
    }
   ],
   "source": [
    "it, reduced_model, *_ = itpruner.get_best_result()\n",
    "with torch.no_grad():\n",
    "    loss_after_prune = loss_fn(reduced_model(X), y)\n",
    "    acc_end_prune = accuracy(reduced_model(X), y)\n",
    "\n",
    "print(f\"best iteration {it}\")\n",
    "print(f\"loss change after prune {loss_end_train- loss_after_prune}\")\n",
    "print(f\"accuracy after prune: {acc_end_prune}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.5000)"
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.isclose(reduced_model[0].weight, torch.tensor(0.0)))/reduced_model[0].weight.size().numel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "from actuallysparse import converter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "sparse_model = nn.Sequential(\n",
    "    converter.convert(model[0], \"coo\", masks[\"0\"][\"weight\"]),\n",
    "    model[1],\n",
    "    converter.convert(model[2], \"coo\", masks[\"2\"][\"weight\"]),\n",
    "    model[3]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Adam does not support sparse gradients, please consider SparseAdam instead",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [64], line 9\u001B[0m\n\u001B[1;32m      7\u001B[0m     sparse_optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m      8\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m----> 9\u001B[0m     \u001B[43msparse_optimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m     12\u001B[0m     y_pred \u001B[38;5;241m=\u001B[39m sparse_model(X)\n",
      "File \u001B[0;32m~/Pruning_Praca_Inzynierska/venv/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001B[0m, in \u001B[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    138\u001B[0m profile_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptimizer.step#\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.step\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(obj\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(profile_name):\n\u001B[0;32m--> 140\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    141\u001B[0m     obj\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[0;32m~/Pruning_Praca_Inzynierska/venv/lib/python3.10/site-packages/torch/optim/optimizer.py:23\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     22\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m---> 23\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     25\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(prev_grad)\n",
      "File \u001B[0;32m~/Pruning_Praca_Inzynierska/venv/lib/python3.10/site-packages/torch/optim/adam.py:206\u001B[0m, in \u001B[0;36mAdam.step\u001B[0;34m(self, closure, grad_scaler)\u001B[0m\n\u001B[1;32m    204\u001B[0m params_with_grad\u001B[38;5;241m.\u001B[39mappend(p)\n\u001B[1;32m    205\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m p\u001B[38;5;241m.\u001B[39mgrad\u001B[38;5;241m.\u001B[39mis_sparse:\n\u001B[0;32m--> 206\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAdam does not support sparse gradients, please consider SparseAdam instead\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    207\u001B[0m grads\u001B[38;5;241m.\u001B[39mappend(p\u001B[38;5;241m.\u001B[39mgrad)\n\u001B[1;32m    209\u001B[0m state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate[p]\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Adam does not support sparse gradients, please consider SparseAdam instead"
     ]
    }
   ],
   "source": [
    "sparse_optimizer = torch.optim.Adam(sparse_model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    y_pred = sparse_model(X)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "\n",
    "    sparse_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    sparse_optimizer.step()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = sparse_model(X)\n",
    "    loss_fine_tuned = loss_fn(y_pred, y)\n",
    "\n",
    "print(f\"loss change {(loss_pruned - loss_fine_tuned)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[2.2866e-01, 7.7130e-01, 4.2970e-05],\n        [1.2582e-01, 8.7412e-01, 6.1340e-05],\n        [2.0059e-01, 7.9935e-01, 5.2136e-05],\n        [1.7629e-01, 8.2365e-01, 6.1827e-05],\n        [2.6519e-01, 7.3477e-01, 4.1297e-05],\n        [2.8343e-01, 7.1651e-01, 5.4384e-05],\n        [2.6471e-01, 7.3524e-01, 5.6522e-05],\n        [2.1195e-01, 7.8800e-01, 4.7899e-05],\n        [1.4546e-01, 8.5447e-01, 7.1931e-05],\n        [1.4797e-01, 8.5198e-01, 4.9855e-05],\n        [2.3590e-01, 7.6406e-01, 3.9550e-05],\n        [2.3170e-01, 7.6825e-01, 5.1242e-05],\n        [1.3683e-01, 8.6312e-01, 5.2247e-05],\n        [1.9759e-01, 8.0236e-01, 5.0313e-05],\n        [2.5698e-01, 7.4299e-01, 3.0701e-05],\n        [3.4152e-01, 6.5844e-01, 4.1310e-05],\n        [2.9567e-01, 7.0429e-01, 4.4965e-05],\n        [2.3083e-01, 7.6912e-01, 5.0398e-05],\n        [2.2018e-01, 7.7977e-01, 4.6961e-05],\n        [2.9955e-01, 7.0040e-01, 4.5136e-05],\n        [1.6159e-01, 8.3836e-01, 4.9783e-05],\n        [2.8102e-01, 7.1893e-01, 5.5605e-05],\n        [3.2889e-01, 6.7107e-01, 3.6655e-05],\n        [1.6715e-01, 8.3275e-01, 9.8849e-05],\n        [2.1994e-01, 7.8000e-01, 5.8974e-05],\n        [1.1177e-01, 8.8816e-01, 6.5710e-05],\n        [2.0998e-01, 7.8995e-01, 7.1431e-05],\n        [2.1340e-01, 7.8656e-01, 4.4293e-05],\n        [1.9237e-01, 8.0758e-01, 4.4872e-05],\n        [1.8916e-01, 8.1078e-01, 5.9159e-05],\n        [1.5257e-01, 8.4737e-01, 6.2710e-05],\n        [1.6814e-01, 8.3180e-01, 6.5081e-05],\n        [3.2921e-01, 6.7076e-01, 3.1530e-05],\n        [3.2094e-01, 6.7903e-01, 3.2484e-05],\n        [1.4638e-01, 8.5356e-01, 5.9210e-05],\n        [1.7108e-01, 8.2888e-01, 4.8239e-05],\n        [1.8528e-01, 8.1468e-01, 3.9541e-05],\n        [2.7355e-01, 7.2642e-01, 3.6321e-05],\n        [1.7678e-01, 8.2315e-01, 6.3434e-05],\n        [2.0041e-01, 7.9954e-01, 4.7327e-05],\n        [2.4665e-01, 7.5331e-01, 4.8777e-05],\n        [3.4394e-02, 9.6546e-01, 1.4715e-04],\n        [2.3496e-01, 7.6499e-01, 5.4134e-05],\n        [2.3967e-01, 7.6023e-01, 9.9844e-05],\n        [2.9088e-01, 7.0906e-01, 6.4797e-05],\n        [1.3205e-01, 8.6788e-01, 7.5226e-05],\n        [2.9073e-01, 7.0923e-01, 4.0888e-05],\n        [2.0802e-01, 7.9192e-01, 5.5017e-05],\n        [2.4800e-01, 7.5196e-01, 3.9951e-05],\n        [1.8984e-01, 8.1011e-01, 4.8833e-05],\n        [1.8724e-02, 8.6996e-01, 1.1132e-01],\n        [2.9405e-02, 8.1814e-01, 1.5246e-01],\n        [1.4129e-02, 7.0651e-01, 2.7936e-01],\n        [8.6494e-03, 9.3024e-01, 6.1112e-02],\n        [1.0519e-02, 7.4552e-01, 2.4396e-01],\n        [1.8850e-02, 8.9228e-01, 8.8873e-02],\n        [3.5368e-02, 6.5689e-01, 3.0774e-01],\n        [1.4809e-02, 9.7984e-01, 5.3495e-03],\n        [1.2249e-02, 9.0829e-01, 7.9463e-02],\n        [2.4789e-02, 9.0786e-01, 6.7353e-02],\n        [6.3501e-03, 9.8403e-01, 9.6170e-03],\n        [2.8121e-02, 8.4741e-01, 1.2447e-01],\n        [4.5882e-03, 9.8151e-01, 1.3900e-02],\n        [1.6951e-02, 8.0762e-01, 1.7543e-01],\n        [2.8834e-02, 9.5285e-01, 1.8316e-02],\n        [1.8967e-02, 9.0284e-01, 7.8189e-02],\n        [3.2189e-02, 7.5809e-01, 2.0972e-01],\n        [1.3001e-02, 9.7682e-01, 1.0178e-02],\n        [4.9281e-03, 6.9082e-01, 3.0425e-01],\n        [1.0385e-02, 9.7383e-01, 1.5782e-02],\n        [2.0019e-02, 2.9141e-01, 6.8857e-01],\n        [1.4802e-02, 9.5010e-01, 3.5093e-02],\n        [5.7312e-03, 5.6017e-01, 4.3409e-01],\n        [1.3266e-02, 9.1984e-01, 6.6894e-02],\n        [1.4475e-02, 9.3608e-01, 4.9445e-02],\n        [1.6095e-02, 8.9575e-01, 8.8158e-02],\n        [8.6081e-03, 7.9613e-01, 1.9526e-01],\n        [7.8192e-03, 3.4226e-01, 6.4992e-01],\n        [1.8860e-02, 7.6657e-01, 2.1457e-01],\n        [1.2151e-02, 9.8301e-01, 4.8356e-03],\n        [9.2506e-03, 9.7556e-01, 1.5186e-02],\n        [8.9454e-03, 9.8289e-01, 8.1682e-03],\n        [1.4302e-02, 9.6505e-01, 2.0652e-02],\n        [6.6522e-03, 3.3750e-01, 6.5585e-01],\n        [3.7282e-02, 7.4791e-01, 2.1481e-01],\n        [6.3393e-02, 7.1911e-01, 2.1749e-01],\n        [1.7433e-02, 7.6642e-01, 2.1615e-01],\n        [5.2853e-03, 8.9545e-01, 9.9262e-02],\n        [3.4628e-02, 9.2814e-01, 3.7230e-02],\n        [1.2323e-02, 9.3497e-01, 5.2708e-02],\n        [1.3863e-02, 9.3073e-01, 5.5405e-02],\n        [2.2044e-02, 8.4163e-01, 1.3633e-01],\n        [1.1464e-02, 9.6175e-01, 2.6788e-02],\n        [1.1090e-02, 9.8315e-01, 5.7627e-03],\n        [1.6917e-02, 9.2280e-01, 6.0287e-02],\n        [3.0584e-02, 9.4445e-01, 2.4963e-02],\n        [2.4742e-02, 9.2695e-01, 4.8306e-02],\n        [1.6670e-02, 9.3174e-01, 5.1588e-02],\n        [1.7147e-02, 9.7796e-01, 4.8954e-03],\n        [1.9796e-02, 9.3493e-01, 4.5270e-02],\n        [2.6774e-04, 2.6186e-03, 9.9711e-01],\n        [2.6166e-03, 7.9669e-02, 9.1771e-01],\n        [4.1480e-04, 1.4210e-02, 9.8538e-01],\n        [2.2194e-03, 7.3972e-02, 9.2381e-01],\n        [4.1704e-04, 8.9348e-03, 9.9065e-01],\n        [1.4910e-04, 6.4334e-03, 9.9342e-01],\n        [1.2312e-02, 3.7591e-01, 6.1178e-01],\n        [5.5563e-04, 3.1725e-02, 9.6772e-01],\n        [6.8199e-04, 4.8360e-02, 9.5096e-01],\n        [3.3526e-04, 3.5550e-03, 9.9611e-01],\n        [4.1931e-03, 7.9243e-02, 9.1656e-01],\n        [1.4943e-03, 6.6569e-02, 9.3194e-01],\n        [8.1267e-04, 2.3729e-02, 9.7546e-01],\n        [1.5294e-03, 5.1235e-02, 9.4724e-01],\n        [5.3027e-04, 8.2495e-03, 9.9122e-01],\n        [9.7783e-04, 1.3356e-02, 9.8567e-01],\n        [3.0054e-03, 9.8123e-02, 8.9887e-01],\n        [6.5710e-04, 7.4897e-03, 9.9185e-01],\n        [3.9846e-05, 2.1124e-03, 9.9785e-01],\n        [3.9323e-03, 4.5444e-01, 5.4163e-01],\n        [4.4665e-04, 8.4709e-03, 9.9108e-01],\n        [3.3201e-03, 6.8516e-02, 9.2816e-01],\n        [1.3007e-04, 7.8380e-03, 9.9203e-01],\n        [4.0078e-03, 2.0034e-01, 7.9565e-01],\n        [1.3133e-03, 2.1271e-02, 9.7742e-01],\n        [1.7752e-03, 6.1821e-02, 9.3640e-01],\n        [5.9230e-03, 2.3858e-01, 7.5550e-01],\n        [8.8693e-03, 2.2798e-01, 7.6315e-01],\n        [5.5188e-04, 1.6382e-02, 9.8307e-01],\n        [3.1840e-03, 1.9375e-01, 8.0306e-01],\n        [4.1386e-04, 2.5778e-02, 9.7381e-01],\n        [1.9860e-03, 2.7296e-02, 9.7072e-01],\n        [3.9361e-04, 1.0500e-02, 9.8911e-01],\n        [8.7981e-03, 5.2023e-01, 4.7097e-01],\n        [5.0701e-03, 3.8222e-01, 6.1271e-01],\n        [1.4622e-04, 5.7101e-03, 9.9414e-01],\n        [7.9050e-04, 6.7263e-03, 9.9248e-01],\n        [4.1142e-03, 1.0349e-01, 8.9240e-01],\n        [1.0744e-02, 2.5809e-01, 7.3117e-01],\n        [1.1370e-03, 3.0425e-02, 9.6844e-01],\n        [3.2245e-04, 5.8004e-03, 9.9388e-01],\n        [8.4679e-04, 1.8951e-02, 9.8020e-01],\n        [2.6166e-03, 7.9669e-02, 9.1771e-01],\n        [3.5555e-04, 6.3124e-03, 9.9333e-01],\n        [3.2080e-04, 4.0090e-03, 9.9567e-01],\n        [6.3868e-04, 1.4580e-02, 9.8478e-01],\n        [1.7386e-03, 9.7665e-02, 9.0060e-01],\n        [2.1325e-03, 5.7823e-02, 9.4004e-01],\n        [1.6046e-03, 1.3323e-02, 9.8507e-01],\n        [7.4869e-03, 1.6261e-01, 8.2991e-01]], grad_fn=<SoftmaxBackward0>)"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[2.2866e-01, 7.7130e-01, 4.2970e-05],\n        [1.2582e-01, 8.7412e-01, 6.1340e-05],\n        [2.0059e-01, 7.9935e-01, 5.2136e-05],\n        [1.7629e-01, 8.2365e-01, 6.1827e-05],\n        [2.6519e-01, 7.3477e-01, 4.1297e-05],\n        [2.8343e-01, 7.1651e-01, 5.4384e-05],\n        [2.6471e-01, 7.3524e-01, 5.6522e-05],\n        [2.1195e-01, 7.8800e-01, 4.7899e-05],\n        [1.4546e-01, 8.5447e-01, 7.1931e-05],\n        [1.4797e-01, 8.5198e-01, 4.9855e-05],\n        [2.3590e-01, 7.6406e-01, 3.9550e-05],\n        [2.3170e-01, 7.6825e-01, 5.1242e-05],\n        [1.3683e-01, 8.6312e-01, 5.2247e-05],\n        [1.9759e-01, 8.0236e-01, 5.0313e-05],\n        [2.5698e-01, 7.4299e-01, 3.0701e-05],\n        [3.4152e-01, 6.5844e-01, 4.1310e-05],\n        [2.9567e-01, 7.0429e-01, 4.4965e-05],\n        [2.3083e-01, 7.6912e-01, 5.0398e-05],\n        [2.2018e-01, 7.7977e-01, 4.6961e-05],\n        [2.9955e-01, 7.0040e-01, 4.5136e-05],\n        [1.6159e-01, 8.3836e-01, 4.9783e-05],\n        [2.8102e-01, 7.1893e-01, 5.5605e-05],\n        [3.2889e-01, 6.7107e-01, 3.6655e-05],\n        [1.6715e-01, 8.3275e-01, 9.8849e-05],\n        [2.1994e-01, 7.8000e-01, 5.8974e-05],\n        [1.1177e-01, 8.8816e-01, 6.5710e-05],\n        [2.0998e-01, 7.8995e-01, 7.1432e-05],\n        [2.1340e-01, 7.8656e-01, 4.4293e-05],\n        [1.9237e-01, 8.0758e-01, 4.4872e-05],\n        [1.8916e-01, 8.1078e-01, 5.9159e-05],\n        [1.5257e-01, 8.4737e-01, 6.2710e-05],\n        [1.6814e-01, 8.3180e-01, 6.5081e-05],\n        [3.2921e-01, 6.7076e-01, 3.1530e-05],\n        [3.2094e-01, 6.7903e-01, 3.2484e-05],\n        [1.4638e-01, 8.5356e-01, 5.9210e-05],\n        [1.7108e-01, 8.2888e-01, 4.8239e-05],\n        [1.8528e-01, 8.1468e-01, 3.9541e-05],\n        [2.7355e-01, 7.2642e-01, 3.6321e-05],\n        [1.7678e-01, 8.2315e-01, 6.3434e-05],\n        [2.0041e-01, 7.9954e-01, 4.7327e-05],\n        [2.4665e-01, 7.5331e-01, 4.8777e-05],\n        [3.4394e-02, 9.6546e-01, 1.4715e-04],\n        [2.3496e-01, 7.6499e-01, 5.4134e-05],\n        [2.3967e-01, 7.6023e-01, 9.9844e-05],\n        [2.9088e-01, 7.0906e-01, 6.4797e-05],\n        [1.3205e-01, 8.6788e-01, 7.5226e-05],\n        [2.9073e-01, 7.0923e-01, 4.0888e-05],\n        [2.0802e-01, 7.9192e-01, 5.5017e-05],\n        [2.4800e-01, 7.5196e-01, 3.9951e-05],\n        [1.8984e-01, 8.1011e-01, 4.8833e-05],\n        [1.8724e-02, 8.6996e-01, 1.1132e-01],\n        [2.9405e-02, 8.1814e-01, 1.5246e-01],\n        [1.4129e-02, 7.0651e-01, 2.7936e-01],\n        [8.6494e-03, 9.3024e-01, 6.1112e-02],\n        [1.0519e-02, 7.4552e-01, 2.4396e-01],\n        [1.8850e-02, 8.9228e-01, 8.8873e-02],\n        [3.5368e-02, 6.5689e-01, 3.0774e-01],\n        [1.4809e-02, 9.7984e-01, 5.3495e-03],\n        [1.2249e-02, 9.0829e-01, 7.9463e-02],\n        [2.4789e-02, 9.0786e-01, 6.7353e-02],\n        [6.3501e-03, 9.8403e-01, 9.6170e-03],\n        [2.8121e-02, 8.4741e-01, 1.2447e-01],\n        [4.5882e-03, 9.8151e-01, 1.3900e-02],\n        [1.6951e-02, 8.0762e-01, 1.7543e-01],\n        [2.8834e-02, 9.5285e-01, 1.8316e-02],\n        [1.8967e-02, 9.0284e-01, 7.8189e-02],\n        [3.2189e-02, 7.5809e-01, 2.0972e-01],\n        [1.3001e-02, 9.7682e-01, 1.0178e-02],\n        [4.9281e-03, 6.9082e-01, 3.0425e-01],\n        [1.0385e-02, 9.7383e-01, 1.5782e-02],\n        [2.0019e-02, 2.9141e-01, 6.8857e-01],\n        [1.4802e-02, 9.5010e-01, 3.5093e-02],\n        [5.7312e-03, 5.6018e-01, 4.3409e-01],\n        [1.3266e-02, 9.1984e-01, 6.6894e-02],\n        [1.4475e-02, 9.3608e-01, 4.9445e-02],\n        [1.6095e-02, 8.9575e-01, 8.8158e-02],\n        [8.6081e-03, 7.9613e-01, 1.9526e-01],\n        [7.8192e-03, 3.4226e-01, 6.4992e-01],\n        [1.8860e-02, 7.6657e-01, 2.1457e-01],\n        [1.2151e-02, 9.8301e-01, 4.8356e-03],\n        [9.2506e-03, 9.7556e-01, 1.5186e-02],\n        [8.9454e-03, 9.8289e-01, 8.1682e-03],\n        [1.4302e-02, 9.6505e-01, 2.0652e-02],\n        [6.6522e-03, 3.3750e-01, 6.5585e-01],\n        [3.7282e-02, 7.4791e-01, 2.1481e-01],\n        [6.3393e-02, 7.1911e-01, 2.1749e-01],\n        [1.7433e-02, 7.6642e-01, 2.1615e-01],\n        [5.2853e-03, 8.9545e-01, 9.9262e-02],\n        [3.4628e-02, 9.2814e-01, 3.7230e-02],\n        [1.2323e-02, 9.3497e-01, 5.2708e-02],\n        [1.3863e-02, 9.3073e-01, 5.5405e-02],\n        [2.2044e-02, 8.4163e-01, 1.3633e-01],\n        [1.1464e-02, 9.6175e-01, 2.6788e-02],\n        [1.1090e-02, 9.8315e-01, 5.7627e-03],\n        [1.6917e-02, 9.2280e-01, 6.0287e-02],\n        [3.0584e-02, 9.4445e-01, 2.4963e-02],\n        [2.4742e-02, 9.2695e-01, 4.8306e-02],\n        [1.6670e-02, 9.3174e-01, 5.1588e-02],\n        [1.7147e-02, 9.7796e-01, 4.8954e-03],\n        [1.9796e-02, 9.3493e-01, 4.5270e-02],\n        [2.6774e-04, 2.6186e-03, 9.9711e-01],\n        [2.6166e-03, 7.9669e-02, 9.1771e-01],\n        [4.1480e-04, 1.4210e-02, 9.8538e-01],\n        [2.2194e-03, 7.3972e-02, 9.2381e-01],\n        [4.1704e-04, 8.9348e-03, 9.9065e-01],\n        [1.4910e-04, 6.4334e-03, 9.9342e-01],\n        [1.2312e-02, 3.7591e-01, 6.1178e-01],\n        [5.5563e-04, 3.1725e-02, 9.6772e-01],\n        [6.8199e-04, 4.8360e-02, 9.5096e-01],\n        [3.3526e-04, 3.5550e-03, 9.9611e-01],\n        [4.1931e-03, 7.9243e-02, 9.1656e-01],\n        [1.4943e-03, 6.6569e-02, 9.3194e-01],\n        [8.1267e-04, 2.3729e-02, 9.7546e-01],\n        [1.5294e-03, 5.1235e-02, 9.4724e-01],\n        [5.3027e-04, 8.2495e-03, 9.9122e-01],\n        [9.7783e-04, 1.3356e-02, 9.8567e-01],\n        [3.0054e-03, 9.8123e-02, 8.9887e-01],\n        [6.5710e-04, 7.4897e-03, 9.9185e-01],\n        [3.9846e-05, 2.1124e-03, 9.9785e-01],\n        [3.9323e-03, 4.5444e-01, 5.4163e-01],\n        [4.4665e-04, 8.4709e-03, 9.9108e-01],\n        [3.3201e-03, 6.8516e-02, 9.2816e-01],\n        [1.3007e-04, 7.8380e-03, 9.9203e-01],\n        [4.0078e-03, 2.0034e-01, 7.9565e-01],\n        [1.3133e-03, 2.1271e-02, 9.7742e-01],\n        [1.7752e-03, 6.1821e-02, 9.3640e-01],\n        [5.9230e-03, 2.3858e-01, 7.5550e-01],\n        [8.8693e-03, 2.2798e-01, 7.6315e-01],\n        [5.5188e-04, 1.6382e-02, 9.8307e-01],\n        [3.1840e-03, 1.9375e-01, 8.0306e-01],\n        [4.1386e-04, 2.5778e-02, 9.7381e-01],\n        [1.9860e-03, 2.7296e-02, 9.7072e-01],\n        [3.9361e-04, 1.0500e-02, 9.8911e-01],\n        [8.7981e-03, 5.2023e-01, 4.7097e-01],\n        [5.0701e-03, 3.8222e-01, 6.1271e-01],\n        [1.4622e-04, 5.7101e-03, 9.9414e-01],\n        [7.9050e-04, 6.7263e-03, 9.9248e-01],\n        [4.1142e-03, 1.0349e-01, 8.9240e-01],\n        [1.0744e-02, 2.5809e-01, 7.3117e-01],\n        [1.1370e-03, 3.0425e-02, 9.6844e-01],\n        [3.2245e-04, 5.8004e-03, 9.9388e-01],\n        [8.4679e-04, 1.8951e-02, 9.8020e-01],\n        [2.6166e-03, 7.9669e-02, 9.1771e-01],\n        [3.5555e-04, 6.3124e-03, 9.9333e-01],\n        [3.2080e-04, 4.0090e-03, 9.9567e-01],\n        [6.3868e-04, 1.4580e-02, 9.8478e-01],\n        [1.7386e-03, 9.7665e-02, 9.0060e-01],\n        [2.1325e-03, 5.7823e-02, 9.4004e-01],\n        [1.6046e-03, 1.3323e-02, 9.8507e-01],\n        [7.4869e-03, 1.6261e-01, 8.2991e-01]], grad_fn=<SoftmaxBackward0>)"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_model(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}